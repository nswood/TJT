{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeefce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "# Imports basics\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import setGPU\n",
    "import sklearn\n",
    "import corner\n",
    "\n",
    "# Imports neural net tools\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from fast_soft_sort.pytorch_ops import soft_rank\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf55b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting\n",
      "<KeysViewHDF5 ['deepDoubleQ']>\n",
      "(3075307, 207)\n"
     ]
    }
   ],
   "source": [
    "# Opens files and reads data\n",
    "\n",
    "print(\"Extracting\")\n",
    "outdir = 'data/IN_FlatSamples_Pytorch'\n",
    "fOne = h5py.File(\"data/FullQCD_FullSig_Zqq_fillfactor1_pTsdmassfilling_dRlimit08_50particlesordered_bkgFill_genMatched50.h5\", 'r')\n",
    "print(fOne.keys())\n",
    "totalData = fOne[\"deepDoubleQ\"][:]\n",
    "print(totalData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378cd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets controllable values\n",
    "\n",
    "particlesConsidered = 50\n",
    "particlesPostCut = 50\n",
    "entriesPerParticle = 4\n",
    "eventDataLength = 6\n",
    "decayTypeColumn = -1\n",
    "datapoints = 1400000\n",
    "trainingDataLength = int(len(totalData)*0.8)\n",
    "validationDataLength = int(len(totalData)*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685318e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data\n",
      "(2460245, 4, 50)\n",
      "(307530, 4, 50)\n",
      "Selecting particlesPostCut\n"
     ]
    }
   ],
   "source": [
    "# Creates Training Data\n",
    "\n",
    "print(\"Preparing Data\")\n",
    "\n",
    "particleDataLength = particlesConsidered * entriesPerParticle\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(totalData)\n",
    "\n",
    "#trainingDataLength = int(datapoints*0.8)\n",
    "#validationDataLength = int(datapoints*0.1)\n",
    "\n",
    "mask = [i>40 for i in totalData[:, eventDataLength-1]]\n",
    "totalData = totalData[mask]\n",
    "\n",
    "labels = totalData[:, decayTypeColumn:]\n",
    "particleData = totalData[:, eventDataLength:particleDataLength + eventDataLength]\n",
    "eventData = totalData[:, :eventDataLength]\n",
    "jetMassData = totalData[:, eventDataLength-1] #last entry in eventData (zero indexing)\n",
    "\n",
    "\n",
    "######### Training Data ###############\n",
    "eventTrainingData = np.array(eventData[0:trainingDataLength])\n",
    "jetMassTrainingData = np.array(jetMassData[0:trainingDataLength])\n",
    "particleTrainingData = np.transpose(\n",
    "    particleData[0:trainingDataLength, ].reshape(trainingDataLength, \n",
    "                                                 entriesPerParticle, \n",
    "                                                 particlesConsidered),\n",
    "                                                 axes=(0, 1, 2))\n",
    "trainingLabels = np.array([[i, 1-i] for i in labels[0:trainingDataLength]]).reshape((-1, 2))\n",
    "print(particleTrainingData.shape)\n",
    "\n",
    "########## Validation Data ##########\n",
    "eventValidationData = np.array(eventData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "jetMassValidationData = np.array(jetMassData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "particleValidationData = np.transpose(\n",
    "    particleData[trainingDataLength:trainingDataLength + validationDataLength, ].reshape(validationDataLength,\n",
    "                                                                                         entriesPerParticle,\n",
    "                                                                                         particlesConsidered),\n",
    "                                                                                         axes=(0, 1, 2))\n",
    "validationLabels = np.array([[i, 1-i] for i in labels[trainingDataLength:trainingDataLength + validationDataLength]]).reshape((-1, 2))\n",
    "print(particleValidationData.shape)\n",
    "\n",
    "\n",
    "########### Testing Data ############\n",
    "eventTestData = np.array(eventData[trainingDataLength + validationDataLength:])\n",
    "jetMassTestData = np.array(jetMassData[trainingDataLength + validationDataLength:])\n",
    "particleTestData = np.transpose(particleData[trainingDataLength + validationDataLength:,].reshape(\n",
    "    len(particleData) - trainingDataLength - validationDataLength, entriesPerParticle, particlesConsidered),\n",
    "                                axes=(0, 1, 2))\n",
    "testLabels = np.array([[i, 1-i] for i in labels[trainingDataLength + validationDataLength:]]).reshape((-1, 2))\n",
    "\n",
    "print('Selecting particlesPostCut')\n",
    "particleTrainingData = particleTrainingData[:, :particlesPostCut]\n",
    "particleValidationData = particleValidationData[:, :particlesPostCut]\n",
    "particlesTestData = particleTestData[:, :particlesPostCut]\n",
    "\n",
    "particlesConsidered = particlesPostCut\n",
    "\n",
    "# Separating signal and bkg arrays\n",
    "particleTrainingDataSig = particleTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "particleTrainingDataBkg = particleTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "particleValidationDataSig = particleValidationData[validationLabels[:,0].astype(bool)]\n",
    "particleValidationDataBkg = particleValidationData[validationLabels[:,1].astype(bool)]\n",
    "particleTrainingLabelSig = trainingLabels[trainingLabels[:,0].astype(bool)]\n",
    "particleTrainingLabelBkg = trainingLabels[trainingLabels[:,1].astype(bool)]\n",
    "\n",
    "# Jet mass for correlation\n",
    "jetMassTrainingDataSig = jetMassTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "jetMassTrainingDataBkg = jetMassTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "jetMassValidationDataSig = jetMassValidationData[validationLabels[:,0].astype(bool)]\n",
    "jetMassValidationDataBkg = jetMassValidationData[validationLabels[:,1].astype(bool)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4dbcdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the data a bit!\n",
    "plt.figure()\n",
    "plt.hist(jetMassTrainingDataSig, alpha=0.5)\n",
    "plt.hist(jetMassTrainingDataBkg, alpha=0.5)\n",
    "#plt.hist(jetMassValidationDataSig, density=True, alpha=0.5)\n",
    "#plt.hist(jetMassValidationDataBkg, density=True, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4103a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the interaction matrices\n",
    "class GraphNetnoSV(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden, De=5, Do=6, softmax=False):\n",
    "        super(GraphNetnoSV, self).__init__()\n",
    "        self.hidden = int(hidden)\n",
    "        self.P = params\n",
    "        self.Nv = 0 \n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Nt = self.N * self.Nv\n",
    "        self.Ns = self.Nv * (self.Nv - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = De\n",
    "        self.Dx = 0\n",
    "        self.Do = Do\n",
    "        self.S = 0\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "           \n",
    "        self.Ra = torch.ones(self.Dr, self.Nr)\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3 = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        self.fr1_pv = nn.Linear(self.S + self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2_pv = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3_pv = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        \n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + (self.De), self.hidden).cuda()\n",
    "        self.fo2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fo3 = nn.Linear(int(self.hidden/2), self.Do).cuda()\n",
    "        \n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar_pp = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        \n",
    "\n",
    "        ####Final output matrix for particles###\n",
    "        \n",
    "\n",
    "        C = torch.cat([x, Ebar_pp], 1)\n",
    "        del Ebar_pp\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + (self.De))))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "\n",
    "        \n",
    "        #Taking the sum of over each particle/vertex\n",
    "        N = torch.sum(O, dim=1)\n",
    "        del O\n",
    "        \n",
    "        ### Classification MLP ###\n",
    "\n",
    "        N = self.fc_fixed(N)\n",
    "        \n",
    "        if softmax:\n",
    "            N = nn.Softmax(dim=1)(N)\n",
    "        \n",
    "        return self.activation(N)\n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "    \n",
    "    \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, n_targets):\n",
    "        super(DNN, self).__init__()\n",
    "        #self.flat = torch.flatten()\n",
    "        self.f0 = nn.Linear(200, 400).cuda()\n",
    "        self.f0b = nn.Linear(400, 400).cuda()\n",
    "        self.f1 = nn.Linear(400, 100).cuda()\n",
    "        self.f2 = nn.Linear(100, 50).cuda()\n",
    "        self.f3 = nn.Linear(50, 10).cuda()\n",
    "        self.f4 = nn.Linear(10, n_targets).cuda()\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "    def forward(self, x): \n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.f0(x)\n",
    "        x = self.f0b(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.f3(x)\n",
    "        x = self.f4(x)\n",
    "        return(self.activation(x))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_targets):\n",
    "        super(MLP, self).__init__()\n",
    "        self.f1 = nn.Linear(n_inputs, n_inputs).cuda()\n",
    "        self.f2 = nn.Linear(n_inputs, int(n_inputs/2)).cuda()\n",
    "        self.f3 = nn.Linear(int(n_inputs/2), n_targets).cuda()\n",
    "        self.activation = torch.nn.Softmax()\n",
    "    def forward(self, x): \n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.f3(x)\n",
    "        return(self.activation(x))\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_inputs, n_targets):\n",
    "        super(Linear, self).__init__()\n",
    "        self.f1 = nn.Linear(n_inputs, n_targets).cuda()\n",
    "        self.activation = torch.nn.Softmax()\n",
    "    def forward(self, x): \n",
    "        x = self.f1(x)\n",
    "        return(self.activation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28b20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses \n",
    "class BarlowTwinsLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_param=5e-3):\n",
    "        super(BarlowTwinsLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.device = torch.device('cuda:0')\n",
    "\n",
    "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor):\n",
    "        #self.device = (torch.device('cuda')if z_a.is_cuda else torch.device('cpu'))\n",
    "        # normalize repr. along the batch dimension\n",
    "        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n",
    "        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n",
    "\n",
    "        N = z_a.size(0)\n",
    "        D = z_a.size(1)\n",
    "\n",
    "        # cross-correlation matrix\n",
    "        c = torch.mm(z_a_norm.T, z_b_norm) / N # DxD\n",
    "        # loss\n",
    "        c_diff = (c - torch.eye(D, device=self.device)).pow(2) # DxD\n",
    "        # multiply off-diagonal elems of c_diff by lambda\n",
    "        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n",
    "        loss = c_diff.sum()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "# return a flattened view of the off-diagonal elements of a square matrix\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class VICRegLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_param=1,mu_param=1,nu_param=20):\n",
    "        super(VICRegLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.mu_param = mu_param\n",
    "        self.nu_param = nu_param\n",
    "        #self.device = torch.device('cpu')\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.device = (torch.device('cuda')if x.is_cuda else torch.device('cpu'))\n",
    "        \n",
    "        x_scale = x\n",
    "        y_scale = y\n",
    "        repr_loss = F.mse_loss(x_scale, y_scale)\n",
    "        \n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x = x_scale - x_scale.mean(dim=0)\n",
    "        y = y_scale - y_scale.mean(dim=0)\n",
    "        N = x_scale.size(0)\n",
    "        D = x_scale.size(1)\n",
    "        \n",
    "        std_x = torch.sqrt(x_scale.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y_scale.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x_scale.T @ x_scale) / (N - 1)\n",
    "        cov_y = (y_scale.T @ y_scale) / (N - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(D) + off_diagonal(cov_y).pow_(2).sum().div(D)\n",
    "\n",
    "        #loss = (self.lambda_param * repr_loss + self.mu_param * std_loss+ self.nu_param * cov_loss)\n",
    "        #print(repr_loss,cov_loss,std_loss)\n",
    "        return repr_loss,cov_loss,std_loss\n",
    "    \n",
    "class CorrLoss(nn.Module):\n",
    "    def __init__(self, corr=False,sort_tolerance=1.0,sort_reg='l2'):\n",
    "        super(CorrLoss, self).__init__()\n",
    "        self.tolerance = sort_tolerance\n",
    "        self.reg       = sort_reg\n",
    "        self.corr      = corr\n",
    "        \n",
    "    def spearman(self, pred, target):\n",
    "        pred   = soft_rank(pred.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        target = soft_rank(target.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        #pred   = torchsort.soft_rank(pred.reshape(1,-1),regularization_strength=x)\n",
    "        #target = torchsort.soft_rank(target.reshape(1,-1),regularization_strength=x)\n",
    "        pred = pred - pred.mean()\n",
    "        pred = pred / pred.norm()\n",
    "        target = target - target.mean()\n",
    "        target = target / target.norm()\n",
    "        ret = (pred * target).sum()\n",
    "        if self.corr:\n",
    "            return (1-ret)*(1-ret)\n",
    "        else:\n",
    "            return ret*ret \n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        return self.spearman(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c614dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Loop Barlow DNN JUST Encoder - Just a testing step! does not include classifier #########\n",
    "\n",
    "batchSize = 6000\n",
    "n_targets = 8\n",
    "n_epochs = 100\n",
    "CorrDim = 1\n",
    "label='contrastiveVICReg_encoder'\n",
    "modelName = \"IN_FlatSamples_NoFill_50particles_dRlimit08\" + label\n",
    "\n",
    "#gnn = GraphNetnoSV(particlesPostCut, n_targets, entriesPerParticle, 15,\n",
    "#                      De=5,\n",
    "#                      Do=6, softmax=False)\n",
    "\n",
    "encoder = DNN(n_targets)\n",
    "    \n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "clr_criterion   = VICRegLoss(lambda_param=1,mu_param=1,nu_param=1)\n",
    "cor_criterion  = CorrLoss()\n",
    "acr_criterion  = CorrLoss(corr=True)\n",
    "\n",
    "BarlowLoss = True\n",
    "\n",
    "optimizer = optim.Adam(encoder.parameters(), lr = 0.001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_std_training = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "loss_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "acc_vals_training = np.zeros(n_epochs)\n",
    "acc_vals_validation = np.zeros(n_epochs)\n",
    "acc_std_training = np.zeros(n_epochs)\n",
    "acc_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "final_epoch = 0\n",
    "l_val_best = 99999\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "    particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "    particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                jetMassValidationDataSig)\n",
    "    particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                jetMassValidationDataBkg)\n",
    "    \n",
    "    weightClr = 10\n",
    "    weightCorr1 = 1\n",
    "   \n",
    "    for i in tqdm(range(int(0.8*datapoints/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        trainingvSig = torch.FloatTensor(particleTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig = torch.FloatTensor(jetMassTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1 = torch.cat((trainingvSig[:int(batchSize/2)], \n",
    "                                trainingvBkg[:int(batchSize/2)]))\n",
    "        trainingv1_mass = torch.cat((trainingvMassSig[:int(batchSize/2)], \n",
    "                                trainingvMassBkg[:int(batchSize/2)]))\n",
    "        trainingv2 = torch.cat((trainingvSig[int(batchSize/2):], \n",
    "                                trainingvBkg[int(batchSize/2):]))\n",
    "        trainingv2_mass = torch.cat((trainingvMassSig[int(batchSize/2):], \n",
    "                                trainingvMassBkg[int(batchSize/2):]))\n",
    "        \n",
    "        # Calculate network output\n",
    "        out1 = encoder(trainingv1)\n",
    "        out2 = encoder(trainingv2)\n",
    "        \n",
    "        # VICReg Loss\n",
    "        repr_loss, cov_loss, std_loss = weightClr*clr_criterion(out1, out2)\n",
    "        \n",
    "        l = repr_loss + cov_loss + std_loss\n",
    "       \n",
    "        # AntiCorrelation\n",
    "        for dim in range(CorrDim): \n",
    "            l += weightCorr1*(dim+1)*acr_criterion(trainingv1_mass, out1[:,dim])\n",
    "            l += weightCorr1*(dim+1)*acr_criterion(trainingv2_mass, out2[:,dim])\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1.shape[1]-CorrDim): \n",
    "            l += (dim+1)*cor_criterion(out1[:,dim+CorrDim], trainingv1_mass)\n",
    "            l += (dim+1)*cor_criterion(out2[:,dim+CorrDim], trainingv2_mass)\n",
    "        \n",
    "        #l = weightClr*lossClr  + weightCorr1*lossCorr1 + weightCorr2*lossCorr2 + lossCorr3\n",
    "        \n",
    "        # Classical BCE loss\n",
    "        #trainingv = torch.FloatTensor(particleTrainingData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #out = encoder(trainingv)\n",
    "        #targetv = torch.FloatTensor(trainingLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #l = loss(out, targetv)\n",
    "        \n",
    "        \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        del trainingvSig, trainingvBkg, trainingv1_mass, l, trainingv1, trainingv2, out1, out2\n",
    "        torch.cuda.empty_cache()\n",
    "                   \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    for i in range(int(0.1*datapoints/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        trainingvSig_val = torch.FloatTensor(particleValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig_val = torch.FloatTensor(jetMassValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg_val = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1_val = torch.cat((trainingvSig_val[:int(batchSize/2)], trainingvBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val = torch.cat((trainingvSig_val[int(batchSize/2):], trainingvBkg_val[int(batchSize/2):]))\n",
    "        trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(batchSize/2)], \n",
    "                                trainingvMassBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val_mass = torch.cat((trainingvMassSig_val[int(batchSize/2):], \n",
    "                                trainingvMassBkg_val[int(batchSize/2):]))\n",
    "        \n",
    "        # Barlow Loss\n",
    "        out1_val = encoder(trainingv1_val)\n",
    "        out2_val = encoder(trainingv2_val)\n",
    "        \n",
    "        # VICReg Loss\n",
    "        repr_loss, cov_loss, std_loss = weightClr*clr_criterion(out1_val, out2_val)\n",
    "        l_val = repr_loss + cov_loss + std_loss\n",
    "       \n",
    "        # AntiCorrelation\n",
    "        for dim in range(CorrDim): \n",
    "            l_val += weightCorr1*(dim+1)*acr_criterion(trainingv1_val_mass, out1_val[:,dim])\n",
    "            l_val += weightCorr1*(dim+1)*acr_criterion(trainingv2_val_mass, out2_val[:,dim])\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1_val.shape[1]-CorrDim): \n",
    "            l_val += (dim+1)*cor_criterion(out1_val[:,dim+CorrDim], trainingv1_val_mass)\n",
    "            l_val += (dim+1)*cor_criterion(out2_val[:,dim+CorrDim], trainingv2_val_mass)\n",
    "        \n",
    "        \n",
    "        # Classical validation\n",
    "        trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = encoder(trainingv_val)\n",
    "        # l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        out1_val = out1_val.cpu().detach().numpy()\n",
    "        trainingv1_val_mass = trainingv1_val_mass.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        del trainingvSig_val, trainingvBkg_val, targetv_val, trainingv1_val, trainingv2_val,out2_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    fig, axs = plt.subplots(n_targets, figsize=(10,50))\n",
    "    for dim in range(out1_val.shape[1]): \n",
    "        axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
    "        #plt.xlabel('%s dimension output'%(dim))\n",
    "        axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n",
    "\n",
    "    #fig.ylabel('sdmass')\n",
    "    fig.savefig('contrastivefig%sIN.jpg'%(dim))\n",
    "\n",
    "    plt.figure()\n",
    "    fig = corner.corner(out1_val[:int(batchSize/2)], color='red')\n",
    "    corner.corner(out1_val[int(batchSize/2):], fig=fig, color='blue')\n",
    "    fig.savefig('corner.jpg')\n",
    "    \n",
    "    del out1_val, trainingv1_val_mass\n",
    "    #targetv_cpu = targetv.cpu().data.numpy()\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(encoder.state_dict(), '%s/encoder_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(encoder.state_dict(), '%s/encoder_%s_best.pth'%(outdir,label))\n",
    "\n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "\n",
    "print('DONE with normal training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 186/186 [01:13<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 75.6265 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149685/1868515072.py:169: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k+\" (-> color='k'). The keyword argument will take precedence.\n",
      "  axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
      "/tmp/ipykernel_149685/1868515072.py:171: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k+\" (-> color='k'). The keyword argument will take precedence.\n",
      "  axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done in 9.3441 seconds\n",
      "\n",
      "Validation Loss:  4.132240025893502\n",
      "Training Loss:  244.43049932295276\n",
      "new best model\n",
      "(138000, 2) (138000, 8)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]] [[0.0779424  0.02412221 0.08131897 ... 0.06781383 0.08602326 0.07755897]\n",
      " [0.0793196  0.0261489  0.08429348 ... 0.07275696 0.09067661 0.07908452]\n",
      " [0.0789305  0.02407712 0.08012024 ... 0.06750259 0.08247694 0.07476884]\n",
      " ...\n",
      " [0.0784496  0.02433399 0.0800768  ... 0.06710884 0.08462635 0.07653619]\n",
      " [0.07984413 0.02482263 0.08075619 ... 0.06906311 0.08411385 0.07447616]\n",
      " [0.0728094  0.02093177 0.07423238 ... 0.06084187 0.07862093 0.06991041]]\n",
      "Validation Accuracy:  0.483195652173913\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 186/186 [01:10<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 73.4396 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149685/1868515072.py:169: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k+\" (-> color='k'). The keyword argument will take precedence.\n",
      "  axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
      "/tmp/ipykernel_149685/1868515072.py:171: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k+\" (-> color='k'). The keyword argument will take precedence.\n",
      "  axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done in 9.7028 seconds\n",
      "\n",
      "Validation Loss:  3.120061086571735\n",
      "Training Loss:  3.4318077141238796\n",
      "new best model\n",
      "(138000, 2) (138000, 8)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]] [[0.06284746 0.01723517 0.06009826 ... 0.05004073 0.069683   0.05643434]\n",
      " [0.06304003 0.01827716 0.06132768 ... 0.05301968 0.07272592 0.05664106]\n",
      " [0.06074095 0.01541113 0.05503973 ... 0.04575805 0.06184188 0.04964103]\n",
      " ...\n",
      " [0.06037869 0.015609   0.05507694 ... 0.0455822  0.06362527 0.05095081]\n",
      " [0.06149436 0.01586105 0.05543491 ... 0.04677515 0.06294642 0.04930471]\n",
      " [0.05751129 0.01422233 0.05293961 ... 0.04300686 0.06129846 0.04855254]]\n",
      "Validation Accuracy:  0.483195652173913\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████████▏                                                                                                                                                                                         | 19/186 [00:07<01:06,  2.51it/s]"
     ]
    }
   ],
   "source": [
    "# Separate Encoder & Classifier training \n",
    "\n",
    "\n",
    "batchSize = 6000\n",
    "n_targets = 8\n",
    "n_epochs = 300\n",
    "CorrDim = 1\n",
    "label='contrastiveVICReg_separate'\n",
    "modelName = \"IN_FlatSamples_NoFill_50particles_dRlimit08\" + label\n",
    "\n",
    "encoder = DNN(n_targets)\n",
    "\n",
    "#gnn = GraphNetnoSV(particlesPostCut, n_targets, entriesPerParticle, 15,\n",
    "#                      De=5,\n",
    "#                      Do=6, softmax=False)\n",
    "\n",
    "\n",
    "\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "clr_criterion  = VICRegLoss(lambda_param=1,mu_param=1,nu_param=1)\n",
    "cor_criterion  = CorrLoss()\n",
    "acr_criterion  = CorrLoss(corr=True)\n",
    "\n",
    "optimizer = optim.Adam(encoder.parameters(), lr = 0.0001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_std_training = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "loss_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "acc_vals_training = np.zeros(n_epochs)\n",
    "acc_vals_validation = np.zeros(n_epochs)\n",
    "acc_std_training = np.zeros(n_epochs)\n",
    "acc_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "final_epoch = 0\n",
    "l_val_best = 99999\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "    particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "    particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                jetMassValidationDataSig)\n",
    "    particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                jetMassValidationDataBkg)\n",
    "    \n",
    "    weightClr = 1\n",
    "    weightrepr = 1\n",
    "    weightcov = 1000\n",
    "    weightstd = 1\n",
    "    weightCorr1 = 1\n",
    "   \n",
    "    for i in tqdm(range(int(0.8*datapoints/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        trainingvSig = torch.FloatTensor(particleTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig = torch.FloatTensor(jetMassTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1 = torch.cat((trainingvSig[:int(batchSize/2)], \n",
    "                                trainingvBkg[:int(batchSize/2)]))\n",
    "        trainingv1_mass = torch.cat((trainingvMassSig[:int(batchSize/2)], \n",
    "                                trainingvMassBkg[:int(batchSize/2)]))\n",
    "        trainingv2 = torch.cat((trainingvSig[int(batchSize/2):], \n",
    "                                trainingvBkg[int(batchSize/2):]))\n",
    "        trainingv2_mass = torch.cat((trainingvMassSig[int(batchSize/2):], \n",
    "                                trainingvMassBkg[int(batchSize/2):]))\n",
    "        \n",
    "        # Calculate network output\n",
    "        out1 = encoder(trainingv1)\n",
    "        out2 = encoder(trainingv2)\n",
    "        \n",
    "        #VICReg Loss\n",
    "        repr_loss, cov_loss, std_loss = clr_criterion(out1, out2)\n",
    "        \n",
    "        l = weightClr*(weightrepr*repr_loss + weightcov*cov_loss + weightstd*std_loss)\n",
    "       \n",
    "        # AntiCorrelation\n",
    "        for dim in range(CorrDim): \n",
    "            l += weightCorr1*(dim+1)*acr_criterion(trainingv1_mass, out1[:,dim])\n",
    "            l += weightCorr1*(dim+1)*acr_criterion(trainingv2_mass, out2[:,dim])\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1.shape[1]-CorrDim): \n",
    "            l += (dim+1)*cor_criterion(out1[:,dim+CorrDim], trainingv1_mass)\n",
    "            l += (dim+1)*cor_criterion(out2[:,dim+CorrDim], trainingv2_mass)\n",
    "            \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        del trainingvSig, trainingvBkg, trainingv1_mass, l, trainingv1, trainingv2, out1, out2\n",
    "        torch.cuda.empty_cache()\n",
    "                   \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    for i in range(int(0.1*datapoints/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        trainingvSig_val = torch.FloatTensor(particleValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig_val = torch.FloatTensor(jetMassValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg_val = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1_val = torch.cat((trainingvSig_val[:int(batchSize/2)], trainingvBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val = torch.cat((trainingvSig_val[int(batchSize/2):], trainingvBkg_val[int(batchSize/2):]))\n",
    "        trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(batchSize/2)], \n",
    "                                trainingvMassBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val_mass = torch.cat((trainingvMassSig_val[int(batchSize/2):], \n",
    "                                trainingvMassBkg_val[int(batchSize/2):]))\n",
    "        \n",
    "        # VICReg Loss\n",
    "        out1_val = encoder(trainingv1_val)\n",
    "        out2_val = encoder(trainingv2_val)\n",
    "        repr_loss, cov_loss, std_loss = clr_criterion(out1_val, out2_val)\n",
    "        \n",
    "        l_val = weightClr*(weightrepr*repr_loss + weightcov*cov_loss + weightstd*std_loss)\n",
    "       \n",
    "        # AntiCorrelation\n",
    "        for dim in range(CorrDim): \n",
    "            l_val += weightCorr1*(dim+1)*acr_criterion(trainingv1_val_mass, out1_val[:,dim])\n",
    "            l_val += weightCorr1*(dim+1)*acr_criterion(trainingv2_val_mass, out2_val[:,dim])\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1_val.shape[1]-CorrDim): \n",
    "            l_val += (dim+1)*cor_criterion(out1_val[:,dim+CorrDim], trainingv1_val_mass)\n",
    "            l_val += (dim+1)*cor_criterion(out2_val[:,dim+CorrDim], trainingv2_val_mass)\n",
    "        \n",
    "        \n",
    "        # Classical validation\n",
    "        trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = encoder(trainingv_val)\n",
    "        # l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        out1_val = out1_val.cpu().detach().numpy()\n",
    "        trainingv1_val_mass = trainingv1_val_mass.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        del trainingvSig_val, trainingvBkg_val, targetv_val, trainingv1_val, trainingv2_val,out2_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    fig, axs = plt.subplots(n_targets, figsize=(10,50))\n",
    "    for dim in range(out1_val.shape[1]): \n",
    "        axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
    "        #plt.xlabel('%s dimension output'%(dim))\n",
    "        axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n",
    "\n",
    "    #fig.ylabel('sdmass')\n",
    "    fig.savefig('contrastivefig%sIN.jpg'%(dim))\n",
    "\n",
    "    plt.figure()\n",
    "    label_str = [\"latent var %s\"%str(i) for i in range(n_targets)]\n",
    "    fig = corner.corner(out1_val[:int(batchSize/2)], color='red', labels=label_str)\n",
    "    corner.corner(out1_val[int(batchSize/2):], fig=fig, color='blue', labels=label_str)\n",
    "    fig.savefig('corner.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    label_str += \"mass\"\n",
    "    fig = corner.corner(np.concatenate((out1_val[:int(batchSize/2)], trainingv1_val_mass[:int(batchSize/2)].reshape(-1, 1)), axis=1), color='red', labels=label_str)\n",
    "    corner.corner(np.concatenate((out1_val[int(batchSize/2):], trainingv1_val_mass[int(batchSize/2):].reshape(-1, 1)), axis=1), fig=fig, color='blue', labels=label_str)\n",
    "    fig.savefig('corner_withMass.jpg')\n",
    "    del out1_val, trainingv1_val_mass\n",
    "    #targetv_cpu = targetv.cpu().data.numpy()\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(encoder.state_dict(), '%s/encoder_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(encoder.state_dict(), '%s/encoder_%s_best.pth'%(outdir,label))\n",
    "\n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "\n",
    "print('DONE with ENCODER training')\n",
    "\n",
    "classifier = MLP(n_targets-CorrDim, 2)\n",
    "loss = nn.BCELoss(reduction='mean')  \n",
    "optimizer = optim.Adam(classifier.parameters(), lr = 0.001)\n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    for i in tqdm(range(int(0.8*datapoints*2/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ######### train classifier #########\n",
    "        trainingv = torch.FloatTensor(particleTrainingData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv = torch.FloatTensor(trainingLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        outC = classifier(encoder(trainingv)[:, CorrDim:])\n",
    "        l = loss(outC, targetv)\n",
    "        \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        del trainingv, targetv\n",
    "        torch.cuda.empty_cache()\n",
    "                   \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    for i in range(int(0.1*datapoints*2/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Classifier \n",
    "        targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = classifier(encoder(trainingv_val)[:, CorrDim:])\n",
    "        \n",
    "        l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        \n",
    "        del trainingv_val, targetv_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(classifier.state_dict(), '%s/classifier_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(classifier.state_dict(), '%s/classifier_%s_best.pth'%(outdir,label))\n",
    "    \n",
    "    \n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "        \n",
    "print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "print('DONE with CLASSIFIER training')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation Cell ##########\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "# Classical validation\n",
    "testv = torch.FloatTensor(particleTestData).cuda()\n",
    "predictions = classifier(encoder(testv)[:, CorrDim:]).cpu().detach().numpy()\n",
    "\n",
    "testData = totalData[trainingDataLength + validationDataLength:, ]\n",
    "#testLabels = np.array(labels[trainingDataLength + validationDataLength:])\n",
    "print(predictions)\n",
    "print(testLabels)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(np.array(testLabels)[:,1].reshape(-1), np.array(predictions)[:,1].reshape(-1))\n",
    "plt.plot(fpr, tpr, lw=2.5, label=\"{}, AUC = {:.1f} %\".format('ZprimeAtoqq IN',auc(fpr,tpr)*100))\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.savefig('data/IN_FlatSamples_Pytorch/{}_model_ROC.jpg'.format(modelName))\n",
    "\n",
    "\n",
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx, array[idx]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(np.array(testLabels)[:,1].reshape(-1), np.array(predictions)[:,1].reshape(-1))\n",
    "cuts = {}\n",
    "for wp in [0.01, 0.03, 0.05, 1.0]:#0.1, 0.5, 1.0]: # % mistag rate\n",
    "    idx, val = find_nearest(fpr, wp)\n",
    "    cuts[str(wp)] = threshold[idx] # threshold for deep double-b corresponding to ~1% mistag rate\n",
    "        \n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "print('sculpting')\n",
    "sculpt_vars = ['jet_eta', \"jet_phi\",\"jet_EhadOverEem\",\"jet_mass\", 'jet_pT', 'jet_sdmass']\n",
    "for i in range(len(sculpt_vars)):\n",
    "    f, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    for wp, cut in reversed(sorted(cuts.items())):\n",
    "        predictions = np.array(predictions)\n",
    "        ctdf = np.array([testData[pred, i] for pred in range(len(predictions)) if predictions[pred,0] > cut])\n",
    "        weight = np.array([testLabels[pred, 1] for pred in range(len(predictions)) if predictions[pred,0] > cut])\n",
    "        \n",
    "        print(ctdf.shape)\n",
    "        print(weight.shape)\n",
    "        if str(wp)=='1.0':\n",
    "            ax.hist(ctdf.flatten(), weights = weight/np.sum(weight), lw=2,\n",
    "                        histtype='step',label='No tagging applied ({} Events)'.format(len(ctdf.flatten())), bins = 10)\n",
    "        else:\n",
    "            ax.hist(ctdf.flatten(), weights = weight/np.sum(weight), lw=2,\n",
    "                        histtype='step',label='{}%  mistagging rate ({} Events)'.format(float(wp)*100., len(ctdf.flatten())), bins=10)\n",
    "\n",
    "    ax.set_xlabel(sculpt_vars[i])\n",
    "    ax.set_ylabel('Normalized Scale QCD')\n",
    "    ax.set_title(sculpt_vars[i] + ' 50 Particle Sculpting') \n",
    "    ax.legend() \n",
    "\n",
    "    f.savefig('data/IN_FlatSamples_Pytorch/{}_sculpting_'.format(modelName) + sculpt_vars[i] + '.jpg')\n",
    "    \n",
    "print(predictions)\n",
    "print(testLabels)\n",
    "print('ptBinning')\n",
    "hist, pt_bins = np.histogram(testData[:, 4], bins=5)\n",
    "for pt_bin in range(len(pt_bins)): \n",
    "    \n",
    "    testDataNew = []\n",
    "    testLabelsNew = []\n",
    "    predictionsNew = []\n",
    "    for i in range(len(testData)): \n",
    "        if testData[i, 4] < pt_bins[pt_bin + 1]: \n",
    "            if testData[i, 4] > pt_bins[pt_bin]:\n",
    "                testDataNew.append(testData[i])\n",
    "                testLabelsNew.append(testLabels[i])\n",
    "                predictionsNew.append(predictions[i])\n",
    "    \n",
    "    testDataNew = np.array(testDataNew)\n",
    "    testLabelsNew = np.array(testLabelsNew)\n",
    "    predictionsNew = np.array(predictionsNew)\n",
    "    \n",
    "    print(testDataNew)\n",
    "    print(testLabelsNew)\n",
    "    print(predictionsNew)\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(np.array(testLabelsNew).reshape(-1), np.array(predictionsNew).reshape(-1))\n",
    "    cuts = {}\n",
    "    for wp in [0.01, 0.03, 0.05, 1.0]:#0.1, 0.5, 1.0]: # % mistag rate\n",
    "        idx, val = find_nearest(fpr, wp)\n",
    "        cuts[str(wp)] = threshold[idx] # threshold for deep double-b corresponding to ~1% mistag rate\n",
    "    \n",
    "    for i in range(len(sculpt_vars)):\n",
    "        f, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "        for wp, cut in reversed(sorted(cuts.items())):\n",
    "            predictions = np.array(predictions)\n",
    "            ctdf = np.array([testDataNew[pred, i] for pred in range(len(predictionsNew)) if predictionsNew[pred,0] > cut])\n",
    "            weight = np.array([testLabelsNew[pred, 1] for pred in range(len(predictionsNew)) if predictionsNew[pred,0] > cut])\n",
    "\n",
    "            if str(wp)=='1.0':\n",
    "                ax.hist(ctdf.flatten(), weights = weight/np.sum(weight), lw=2,\n",
    "                            histtype='step',label='No tagging applied ({} Events)'.format(len(ctdf.flatten())), bins = 10)\n",
    "            else:\n",
    "                ax.hist(ctdf.flatten(), weights = weight/np.sum(weight), lw=2,\n",
    "                            histtype='step',label='{}%  mistagging rate ({} Events)'.format(float(wp)*100., len(ctdf.flatten())), bins=10)\n",
    "\n",
    "        ax.set_xlabel(sculpt_vars[i])\n",
    "        ax.set_ylabel('Normalized Scale QCD')\n",
    "        ax.set_title(sculpt_vars[i] + ' 50 Particle Sculpting sdmass') \n",
    "        ax.legend() \n",
    "\n",
    "        f.savefig('data/IN_FlatSamples_Pytorch/{}_sculpting_PtBin{}_'.format(modelName, pt_bin) + sculpt_vars[i] + '.jpg')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b345b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised training\n",
    "batchSize = 6000\n",
    "n_targets = 2\n",
    "n_epochs = 300\n",
    "\n",
    "#gnn = GraphNetnoSV(particlesPostCut, n_targets, entriesPerParticle, 15,\n",
    "#                      De=5,\n",
    "#                      Do=6, softmax=False)\n",
    "\n",
    "encoder = DNN(n_targets)\n",
    "    \n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "clr_criterion  = BarlowTwinsLoss(lambda_param=1.0)\n",
    "cor_criterion  = CorrLoss()\n",
    "acr_criterion  = CorrLoss(corr=True)\n",
    "\n",
    "BarlowLoss = True\n",
    "\n",
    "optimizer = optim.Adam(encoder.parameters(), lr = 0.0001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_std_training = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "loss_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "acc_vals_training = np.zeros(n_epochs)\n",
    "acc_vals_validation = np.zeros(n_epochs)\n",
    "acc_std_training = np.zeros(n_epochs)\n",
    "acc_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "final_epoch = 0\n",
    "l_val_best = 99999\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "    particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "    particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                jetMassValidationDataSig)\n",
    "    particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                jetMassValidationDataBkg)\n",
    "    \n",
    "    weightClr = 10\n",
    "    weightCorr1 = 100\n",
    "   \n",
    "    for i in tqdm(range(int(0.8*datapoints/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        trainingvSig = torch.FloatTensor(particleTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig = torch.FloatTensor(jetMassTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1 = torch.cat((trainingvSig[:int(batchSize/2)], \n",
    "                                trainingvBkg[:int(batchSize/2)]))\n",
    "        trainingv1_mass = torch.cat((trainingvMassSig[:int(batchSize/2)], \n",
    "                                trainingvMassBkg[:int(batchSize/2)]))\n",
    "        trainingv2 = torch.cat((trainingvSig[int(batchSize/2):], \n",
    "                                trainingvBkg[int(batchSize/2):]))\n",
    "        \n",
    "        # Calculate network output\n",
    "        out1 = encoder(trainingv1)\n",
    "        out2 = encoder(trainingv2)\n",
    "        \n",
    "        # Barlow Loss\n",
    "        lossClr = weightClr*clr_criterion(out1, out2)\n",
    "        \n",
    "        # AntiCorrelation\n",
    "        lossCorr1 = weightCorr1*acr_criterion(trainingv1_mass, out1[:,0])\n",
    "        l = lossClr + lossCorr1\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1.shape[1]-1): \n",
    "            l += (dim+1)*cor_criterion(out1[:,dim+1], trainingv1_mass)\n",
    "        \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        del trainingvSig, trainingvBkg, trainingv1_mass, l, trainingv1, trainingv2, out1, out2\n",
    "        torch.cuda.empty_cache()\n",
    "                   \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    for i in range(int(0.1*datapoints/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        trainingvSig_val = torch.FloatTensor(particleValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig_val = torch.FloatTensor(jetMassValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg_val = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1_val = torch.cat((trainingvSig_val[:int(batchSize/2)], trainingvBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val = torch.cat((trainingvSig_val[int(batchSize/2):], trainingvBkg_val[int(batchSize/2):]))\n",
    "        trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(batchSize/2)], \n",
    "                                trainingvMassBkg_val[:int(batchSize/2)]))\n",
    "        \n",
    "        # Barlow Loss\n",
    "        out1_val = encoder(trainingv1_val)\n",
    "        out2_val = encoder(trainingv2_val)\n",
    "        lossClr = weightClr*clr_criterion(out1_val, out2_val)\n",
    "        \n",
    "        # AntiCorrelation\n",
    "        lossCorr1 = weightCorr1*acr_criterion(trainingv1_val_mass, out1_val[:,0])\n",
    "        l_val = lossClr + lossCorr1\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1_val.shape[1]-1): \n",
    "            l_val += (dim+1)*cor_criterion(out1_val[:,dim+1], trainingv1_val_mass)\n",
    "        \n",
    "        \n",
    "        # Classical validation\n",
    "        trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = encoder(trainingv_val)\n",
    "        # l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        out1_val = out1_val.cpu().detach().numpy()\n",
    "        trainingv1_val_mass = trainingv1_val_mass.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        del trainingvSig_val, trainingvBkg_val, targetv_val, trainingv1_val, trainingv2_val,out2_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    fig, axs = plt.subplots(n_targets, figsize=(10,50))\n",
    "    for dim in range(out1_val.shape[1]): \n",
    "        axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
    "        #plt.xlabel('%s dimension output'%(dim))\n",
    "        axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n",
    "\n",
    "    #fig.ylabel('sdmass')\n",
    "    fig.savefig('contrastivefig%sIN.jpg'%(dim))\n",
    "\n",
    "    plt.figure()\n",
    "    fig = corner.corner(out1_val[:int(batchSize/2)], color='red')\n",
    "    corner.corner(out1_val[int(batchSize/2):], fig=fig, color='blue')\n",
    "    fig.savefig('corner.jpg')\n",
    "    \n",
    "    del out1_val, trainingv1_val_mass\n",
    "    #targetv_cpu = targetv.cpu().data.numpy()\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(encoder.state_dict(), '%s/encoder_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(encoder.state_dict(), '%s/encoder_%s_best.pth'%(outdir,label))\n",
    "\n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "\n",
    "print('DONE with ENCODER training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Loop Barlow IN #########\n",
    "\n",
    "batchSize = 6000\n",
    "n_targets = 5\n",
    "n_epochs = 100\n",
    "\n",
    "gnn = GraphNetnoSV(particlesPostCut, n_targets, entriesPerParticle, 15,\n",
    "                      De=5,\n",
    "                      Do=6, softmax=False)\n",
    "    \n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "clr_criterion  = BarlowTwinsLoss(lambda_param=1.0)\n",
    "cor_criterion  = CorrLoss()\n",
    "acr_criterion  = CorrLoss(corr=True)\n",
    "\n",
    "BarlowLoss = True\n",
    "\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
    "loss_vals_training = np.zeros(n_epochs)\n",
    "loss_std_training = np.zeros(n_epochs)\n",
    "loss_vals_validation = np.zeros(n_epochs)\n",
    "loss_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "acc_vals_training = np.zeros(n_epochs)\n",
    "acc_vals_validation = np.zeros(n_epochs)\n",
    "acc_std_training = np.zeros(n_epochs)\n",
    "acc_std_validation = np.zeros(n_epochs)\n",
    "\n",
    "final_epoch = 0\n",
    "l_val_best = 99999\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "for m in range(n_epochs):\n",
    "    print(\"Epoch %s\\n\" % m)\n",
    "    #torch.cuda.empty_cache()\n",
    "    final_epoch = m\n",
    "    lst = []\n",
    "    loss_val = []\n",
    "    loss_training = []\n",
    "    correct = []\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "    particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "    particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                jetMassValidationDataSig)\n",
    "    particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                jetMassValidationDataBkg)\n",
    "    \n",
    "    weightClr = 10\n",
    "    weightCorr1 = 100\n",
    "   \n",
    "    for i in tqdm(range(int(0.8*datapoints/batchSize))): \n",
    "        #print('%s out of %s'%(i, int(particleTrainingData.shape[0]/batchSize)))\n",
    "        optimizer.zero_grad()\n",
    "        trainingvSig = torch.FloatTensor(particleTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig = torch.FloatTensor(jetMassTrainingDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1 = torch.cat((trainingvSig[:int(batchSize/2)], \n",
    "                                trainingvBkg[:int(batchSize/2)]))\n",
    "        trainingv1_mass = torch.cat((trainingvMassSig[:int(batchSize/2)], \n",
    "                                trainingvMassBkg[:int(batchSize/2)]))\n",
    "        trainingv2 = torch.cat((trainingvSig[int(batchSize/2):], \n",
    "                                trainingvBkg[int(batchSize/2):]))\n",
    "        \n",
    "        # Calculate network output\n",
    "        out1 = gnn(trainingv1)\n",
    "        out2 = gnn(trainingv2)\n",
    "        print(out1)\n",
    "        # Barlow Loss\n",
    "        lossClr = torch.nan_to_num(weightClr*clr_criterion(out1, out2))\n",
    "        \n",
    "        # AntiCorrelation\n",
    "        lossCorr1 = torch.nan_to_num(weightCorr1*acr_criterion(trainingv1_mass, out1[:,0]))\n",
    "        l = lossClr + lossCorr1\n",
    "        print(lossClr)\n",
    "        print(lossCorr1)\n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1.shape[1]-1): \n",
    "            print(torch.nan_to_num((dim+1)*cor_criterion(out1[:,dim+1], trainingv1_mass)))\n",
    "            l += torch.nan_to_num((dim+1)*cor_criterion(out1[:,dim+1], trainingv1_mass))\n",
    "        \n",
    "        #l = weightClr*lossClr  + weightCorr1*lossCorr1 + weightCorr2*lossCorr2 + lossCorr3\n",
    "        \n",
    "        # Classical BCE loss\n",
    "        #trainingv = torch.FloatTensor(particleTrainingData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #out = gnn(trainingv)\n",
    "        #targetv = torch.FloatTensor(trainingLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        #l = loss(out, targetv)\n",
    "        \n",
    "        \n",
    "        loss_training.append(l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "        del trainingvSig, trainingvBkg, trainingv1_mass, l, trainingv1, trainingv2, out1, out2\n",
    "        torch.cuda.empty_cache()\n",
    "                   \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    for i in range(int(0.1*datapoints/(batchSize))): \n",
    "        torch.cuda.empty_cache()\n",
    "        trainingvSig_val = torch.FloatTensor(particleValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassSig_val = torch.FloatTensor(jetMassValidationDataSig[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingvMassBkg_val = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        trainingv1_val = torch.cat((trainingvSig_val[:int(batchSize/2)], trainingvBkg_val[:int(batchSize/2)]))\n",
    "        trainingv2_val = torch.cat((trainingvSig_val[int(batchSize/2):], trainingvBkg_val[int(batchSize/2):]))\n",
    "        trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(batchSize/2)], \n",
    "                                trainingvMassBkg_val[:int(batchSize/2)]))\n",
    "        \n",
    "        # Barlow Loss\n",
    "        out1_val = gnn(trainingv1_val)\n",
    "        out2_val = gnn(trainingv2_val)\n",
    "        lossClr = torch.nan_to_num(weightClr*clr_criterion(out1_val, out2_val))\n",
    "        \n",
    "        # AntiCorrelation\n",
    "        lossCorr1 = torch.nan_to_num(weightCorr1*acr_criterion(trainingv1_val_mass, out1_val[:,0]))\n",
    "        l_val = lossClr + lossCorr1\n",
    "       \n",
    "        # Correlation for rest of dimensions\n",
    "        for dim in range(out1_val.shape[1]-1): \n",
    "            l_val += (dim+1)*cor_criterion(out1_val[:,dim+1], trainingv1_val_mass)\n",
    "        \n",
    "        \n",
    "        # Classical validation\n",
    "        trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "        out = gnn(trainingv_val)\n",
    "        # l_val = loss(out, targetv_val)\n",
    "        lst.append(out.cpu().data.numpy())\n",
    "        loss_val.append(l_val.item())\n",
    "        correct.append(targetv_val.cpu())\n",
    "        out1_val = out1_val.cpu().detach().numpy()\n",
    "        trainingv1_val_mass = trainingv1_val_mass.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        del trainingvSig_val, trainingvBkg_val, targetv_val, trainingv1_val, trainingv2_val,out2_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    fig, axs = plt.subplots(n_targets, figsize=(10,50))\n",
    "    for dim in range(out1_val.shape[1]): \n",
    "        axs[dim].plot(out1_val[:int(batchSize/2), dim], trainingv1_val_mass[:int(batchSize/2)], 'k+',c='r', alpha=0.5)\n",
    "        #plt.xlabel('%s dimension output'%(dim))\n",
    "        axs[dim].plot(out1_val[int(batchSize/2):, dim], trainingv1_val_mass[int(batchSize/2):], 'k+',c='b', alpha=0.5)\n",
    "\n",
    "    #fig.ylabel('sdmass')\n",
    "    fig.savefig('contrastivefig%sIN.jpg'%(dim))\n",
    "\n",
    "    plt.figure()\n",
    "    fig = corner.corner(out1_val[:int(batchSize/2)], color='red')\n",
    "    corner.corner(out1_val[int(batchSize/2):], fig=fig, color='blue')\n",
    "    fig.savefig('corner.jpg')\n",
    "    \n",
    "    del out1_val, trainingv1_val_mass\n",
    "    #targetv_cpu = targetv.cpu().data.numpy()\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "    l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "    predicted = np.concatenate(lst) #(torch.FloatTensor(np.concatenate(lst))).to(device)\n",
    "    print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "    l_training = np.mean(np.array(loss_training))\n",
    "    print('Training Loss: ', l_training)\n",
    "    val_targetv = np.concatenate(correct) #torch.FloatTensor(np.array(correct)).cuda()\n",
    "\n",
    "    torch.save(gnn.state_dict(), '%s/gnn_%s_last.pth'%(outdir,label))\n",
    "    if l_val < l_val_best:\n",
    "        print(\"new best model\")\n",
    "        l_val_best = l_val\n",
    "        torch.save(gnn.state_dict(), '%s/gnn_%s_best.pth'%(outdir,label))\n",
    "\n",
    "    print(val_targetv.shape, predicted.shape)\n",
    "    print(val_targetv, predicted)\n",
    "    acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "    print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "    loss_vals_training[m] = l_training\n",
    "    loss_vals_validation[m] = l_val\n",
    "    loss_std_validation[m] = np.std(np.array(loss_val))\n",
    "    loss_std_training[m] = np.std(np.array(loss_training))\n",
    "    if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "        print('Early Stopping...')\n",
    "        print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "        break\n",
    "\n",
    "print('DONE with normal training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Accuracy: \", acc_vals_validation[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204bab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN_torch",
   "language": "python",
   "name": "in_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
